{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "import os, subprocess, glob, re, gzip, math, shutil, itertools, sys, time, math, string\n",
    "sys.path.append('scripts/') \n",
    "import jupyter_utils\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome! This notebook serves to sequence design to the RL-generated backbones for capsid formation.\n",
    "\n",
    "You can run the example setup below on a single CPU, but if you want to generate >100 designs that pass stringent filters, you will need access to a high-performance computing system. The pipeline below assumes that your computer cluster uses slurm. For the example, you can just run the first command in the cmdsfile. To submit to your cluster you can use the make_submit_file command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Directories and executables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need to gather the 6 scripts/executables/apptainer below. PyRosetta/Rosetta will require a license (free for academics). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Executables\n",
    "\n",
    "# https://www.rosettacommons.org/software/license-and-download \n",
    "rosexe = 'ROSETTA_PATH/main/source/bin/rosetta_scripts.linuxgccrelease'\n",
    "\n",
    "# https://github.com/rdkibler/superfold_public\n",
    "superfold_exe = '/software/lab/superfold/superfold' \n",
    "\n",
    "# https://pymol.org/\n",
    "pymol = '/usr/bin/pymol' \n",
    "\n",
    "# Apptainer, https://files.ipd.uw.edu/pub/protein-backbone-MCTS/protein-backbone-MCTS.sif\n",
    "RL_apptainer = 'protein-backbone-MCTS.sif' \n",
    "\n",
    "# Sequence profile generator, https://github.com/LongxingCao/profile_prediction\n",
    "profileMaker_script = 'scripts/profile_prediction/profile_prediction.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we setup a bunch of folders. If you are generating backbones from scratch place them in the original_docks directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### directories\n",
    "run_dir = 'test/'   \n",
    "current_dir = os.getcwd()  \n",
    "\n",
    "# backbones resulting from the RL generator ()\n",
    "original_docks = run_dir + 'scaffolds/*/*pdb'\n",
    "\n",
    "# extracted chain A from the RL generator\n",
    "chA_dir = run_dir + 'input/chain_A'\n",
    "extracted_interfaces_dir = run_dir + 'input/small_pdbs'\n",
    "\n",
    "# rainity_pssms\n",
    "chA_pssm_dir = run_dir + 'pssm/pssm_rainity_final_chA'\n",
    "int_pssm_dir = run_dir + 'pssm/pssm_rainity_final_int'\n",
    "\n",
    "# designed monomers\n",
    "designed_monomers_dir = run_dir + 'out/designed_chA'\n",
    "\n",
    "# all designed monomers\n",
    "designed_monomers_all_dir = run_dir + 'out/designed_chA_all' \n",
    "\n",
    "# designed monomers that pass structure score filters\n",
    "filtered_designed_monomers_dir = run_dir + 'out/filtered_designed_chA'\n",
    "\n",
    "# AF_predicted_monomers (AF = Alphafold2)\n",
    "AF_monomers_dir = run_dir + 'AF/designed_chA_AF'\n",
    "\n",
    "# aligned DM monomers\n",
    "aligned_DM_monomers_dir = run_dir + 'monomers_fordesign/aligned_DM_chA'\n",
    "\n",
    "#aligned AF monomers\n",
    "aligned_AF_monomers_dir = run_dir + 'monomers_fordesign/aligned_AF_chA'\n",
    "\n",
    "# cage design for DM round #1 \n",
    "predesigned_DM_cages_dir = run_dir + 'cage_design/1st_DM'\n",
    "\n",
    "# all DM cages chA\n",
    "predesigned_DM_cages_all_chA_dir = run_dir + 'cage_design/1st_DM_chA_all'\n",
    "\n",
    "#cage design for AF round #1 \n",
    "predesigned_AF_cages_dir = run_dir + 'cage_design/1st_AF'\n",
    "#all AF cages chA\n",
    "predesigned_AF_cages_all_chA_dir = run_dir + 'cage_design/1st_AF_chA_all'\n",
    "\n",
    "#interfaces DM\n",
    "extracted_DM_interfaces_dir = run_dir + 'cage_design/1st_DM_interfaces'\n",
    "\n",
    "#interfaces AF\n",
    "extracted_AF_interfaces_dir = run_dir + 'cage_design/1st_AF_interfaces'\n",
    "\n",
    "# scores\n",
    "sc_dir = 'scores'\n",
    "os.makedirs(sc_dir,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prepare scaffolds for design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your RL-generated input scaffolds are (likely) icosahedral capsids. To make it computationally more feasible to compute a sequence design guide (aka position specific scoring matrix, PSSM), we extract chain A in one pdb, and chain A and it's neighbors in another pdb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    }
   ],
   "source": [
    "#################################################################################\n",
    "# generate bash cmd file to extract chain A and chains surrounding chain A\n",
    "#################################################################################\n",
    "\n",
    "# scaffolds for design\n",
    "original_docks = glob.glob(run_dir + 'scaffolds/*/*pdb')\n",
    "print(len(original_docks))\n",
    "\n",
    "os.makedirs(chA_dir, exist_ok=True)\n",
    "os.makedirs(extracted_interfaces_dir, exist_ok=True)\n",
    "os.makedirs('cmds',exist_ok=True)\n",
    "cmdsfile = 'cmds/cmds_extract_chains'\n",
    "with open(cmdsfile, 'w') as f_out:\n",
    "    for pdb in original_docks:\n",
    "        cmd = f\"/usr/bin/pymol -c scripts/extract_chains.py --pdb_in {pdb} --pdb_out {extracted_interfaces_dir}/{pdb.split('/')[-1]}\"\n",
    "        f_out.write(cmd+'\\n')\n",
    "        \n",
    "        cmd = f\"/usr/bin/pymol -c scripts/extract_chains.py --only_chainA 1 --pdb_in {pdb} --pdb_out {chA_dir}/{pdb.split('/')[-1]}\"\n",
    "        f_out.write(cmd+'\\n')\n",
    "\n",
    "submitfile = cmdsfile+'.sh'\n",
    "#jupyter_utils.make_submit_file(cmds=cmdsfile, submitfile=submitfile, group_size=100)\n",
    "#p = subprocess.Popen(['sbatch', submitfile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Make Rainity PSSMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compute the sequence design guide. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34, 34)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of input files \n",
    "pdb_chA = glob.glob(chA_dir + '/' + '*pdb')\n",
    "pdb_int = glob.glob(extracted_interfaces_dir + '/' + '*pdb')\n",
    "len(pdb_int), len(pdb_chA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmdsfile = 'cmds/cmds_profile_gen'\n",
    "os.makedirs(chA_pssm_dir, exist_ok=True)\n",
    "os.makedirs(int_pssm_dir, exist_ok=True)\n",
    "\n",
    "# Remember to conda activate the profileMaker_script python environment before running the script below\n",
    "with open(cmdsfile, 'w') as f_out:\n",
    "    for pdb in pdb_chA: # {pdbname.replace('.pdb','.pssm')}\n",
    "        pssm_name = chA_pssm_dir + '/' + pdb.split('/')[-1][:-4] + '.pssm'\n",
    "        cmd = f'python {profileMaker_script} -pdbs {pdb} -pssm_outname {pssm_name}'\n",
    "        f_out.write(f'{cmd} \\n')\n",
    "    for pdb in pdb_int:\n",
    "        pssm_name = int_pssm_dir + '/' + pdb.split('/')[-1][:-4] + '.pssm'\n",
    "        cmd = f'python {profileMaker_script} -pdbs {pdb} -pssm_outname {pssm_name}' # Automatically selects the PSSM for chain A\n",
    "        f_out.write(f'{cmd} \\n')\n",
    "        \n",
    "#submitfile = cmdsfile+'.sh'\n",
    "#jupyter_utils.make_submit_file(cmds=cmdsfile, submitfile=submitfile, group_size=1, mem='2G', queue='short')\n",
    "#p = subprocess.Popen(['sbatch', submitfile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "now you should have generated a sequence profile file (pssm) for each of your starting backbones in the directories: test/pssm/pssm_rainity_final_chA and test/pssm/pssm_rainity_final_int, respectively.\n"
     ]
    }
   ],
   "source": [
    "print(f'now you should have generated a sequence profile file (pssm) for each of your starting backbones in the directories: {chA_pssm_dir} and {int_pssm_dir}, respectively.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Design the monomers with rainity pssm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The majority of your RL-generated scaffolds will not work because the backbone cannot be designed to form a well-packed hydrophobic core. To avoid spending time on these hopeless cases, we first seek to filter the backbone library by designing and scoring scaffolds as monomers. We start with monomer design as it is computationally more efficient than designing the full capsid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate cmd file to design monomers with chA pdb and pssm file\n",
    "### You need to install Rosetta and update the following two flags in the flags file appropriately:\n",
    "## \"-database ROSETTA_PATH/main/database\" \n",
    "## \"-holes:dalphaball ROSETTA_PATH/main/source/external/DAlpahBall.gcc\"\n",
    "\n",
    "###############################\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "os.makedirs(designed_monomers_dir, exist_ok=True)\n",
    "chA_pdbs = glob.glob(chA_dir + '/*pdb') # input pdbs (chainA)\n",
    "\n",
    "cmdsfile = 'cmds/cmds_design_monomer'\n",
    "flags = f'{current_dir}/protocols/flags'\n",
    "xml = f'{current_dir}/protocols/design_monomer.xml'\n",
    "\n",
    "with open(cmdsfile, 'w') as f_cmds:\n",
    "    for pdb in chA_pdbs:\n",
    "        pdbname = pdb.split('/')[-1][:-4]\n",
    "        pssm_f = f'{current_dir}/{chA_pssm_dir}/{pdbname}.pssm' \n",
    "        chainA = current_dir + '/' + chA_dir + '/' + pdb.split('/')[-1]\n",
    "        design_folder = f'{designed_monomers_dir}/{pdbname}'\n",
    "\n",
    "        os.makedirs(design_folder, exist_ok=True)\n",
    "            \n",
    "        cmd = f'{rosexe} -s {chainA} -parser:protocol {xml} @{flags} -parser:script_vars pssm_f={pssm_f} -out:prefix {design_folder}/rainity_'\n",
    "        f_cmds.write(cmd + '\\n')\n",
    "\n",
    "#submitfile = cmdsfile+'.sh'\n",
    "#jupyter_utils.make_submit_file(cmds=cmdsfile, submitfile=submitfile, group_size=1, mem='2G', queue='short')\n",
    "#p = subprocess.Popen(['sbatch', submitfile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Rosetta filter on monomers - score/res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we gather the scores from design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### collect all score files and print to csv\n",
    "score_paths = glob.glob(designed_monomers_dir + '/*/*_score.sc') \n",
    "\n",
    "dfs = []\n",
    "for score_pathsidx, score_path in enumerate(score_paths):\n",
    "    df_f = pd.read_csv(f'{score_path}',skiprows=1,delim_whitespace=True)\n",
    "    directory = os.path.dirname(score_path)\n",
    "    pdb_path = glob.glob(directory+'/*pdb')[0]\n",
    "    df_f['pdbpath'] = pdb_path\n",
    "    dfs.append(df_f)\n",
    "\n",
    "df=pd.concat(dfs).dropna()\n",
    "df.to_csv(f'{sc_dir}/all_scores_test.csv', index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot overall distribution for key features\n",
    "relevant_features = ['score_per_res','vbuns_all_heavy']\n",
    "for f in relevant_features:\n",
    "    df[f] = df[f].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By manual structural inspection of designed monomers, we settled on the filter values below being reasonable cutoffs. Avoiding buried unsatisfied polar atoms (vbuns) and low energy (score_per_res) seemed reasonable. Everything is passing the ss_count filter (number of secondary structure elements). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### filter by score_per_res, vbuns, and ss_count; and then plot distribution\n",
    "dfsub = df[(df['score_per_res']<-2.7) & (df['vbuns_all_heavy']<1) & (df['ss_count']>2)]\n",
    "\n",
    "for pdb, energy in zip(dfsub['pdbpath'], dfsub['score_per_res']):\n",
    "    fname = pdb.split('/')[-1].replace('rainity_build','build')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we stage the filtered monomers for full capsid design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy to filtered designed monomers\n",
    "os.makedirs(filtered_designed_monomers_dir, exist_ok=True)\n",
    "\n",
    "#dfsub['path'] = designed_monomers_dir + '/' + dfsub['description'].str.replace(\"rainity_\", \"\").str.replace(\"_0001\", \"\") + '/' + dfsub['description'] + '.pdb'\n",
    "\n",
    "# Create a new column with the modified 'description' values\n",
    "dfsub = dfsub.copy()\n",
    "dfsub['modified_description'] = dfsub['description'].str.replace(\"rainity_\", \"\").str.replace(\"_0001\", \"\")\n",
    "dfsub.loc[:, 'path'] = designed_monomers_dir + '/' + dfsub['modified_description'] + '/' + dfsub['description'] + '.pdb'\n",
    "\n",
    "for i,row in dfsub.iterrows():\n",
    "    p = str(row['path'])\n",
    "    d = str(row['description'])\n",
    "    shutil.copy(p, f'{filtered_designed_monomers_dir}/{d}.pdb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Alphafold - plddt and rmsd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another filter, which has proven predictive for scaffold viability, is AlphaFold ability to accurately predict the structure from a single sequence and without templates. Below we compute plddt and RMSD. For convinience, we use a slightly modfied version of AlphaFold2 (superFold)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### superfold filtered designed monomers to get mean pLDDT and rmsd\n",
    "chA_superfold_output_dir = run_dir + \"AF/chA_superfold_output\" ### new_dir\n",
    "os.makedirs(chA_superfold_output_dir, exist_ok=True)\n",
    "\n",
    "cmdsfile = 'cmds/cmds_superfold'\n",
    "with open(cmdsfile, 'w') as f_out:\n",
    "    f_out.write(superfold_exe)\n",
    "    f_out.close()\n",
    "\n",
    "with open(cmdsfile, 'a') as f_out:\n",
    "    for pdb_AF2 in glob.glob(filtered_designed_monomers_dir + '/*pdb'):\n",
    "        cmd = f\"{pdb_AF2}\"\n",
    "        f_out.write(\" \" + cmd)\n",
    "\n",
    "with open(cmdsfile, 'a') as f_out:\n",
    "    f_out.write(\" --out_dir \" + chA_superfold_output_dir + \" --models 1 --output_summary\")\n",
    "    f_out.close\n",
    "\n",
    "#submitfile = cmdsfile+'.sh'\n",
    "#jupyter_utils.make_submit_file(cmds=cmdsfile, submitfile=submitfile, group_size=1000, mem='2G', queue='short')\n",
    "#p = subprocess.Popen(['sbatch', submitfile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we gather the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>model</th>\n",
       "      <th>recycle</th>\n",
       "      <th>tol</th>\n",
       "      <th>mean_plddt</th>\n",
       "      <th>ptM</th>\n",
       "      <th>rmsd</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rainity_build_0.2089_0.98_0.4653_19_2.00_0.005...</td>\n",
       "      <td>model_1_ptm_seed_0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.23</td>\n",
       "      <td>86.44</td>\n",
       "      <td>0.66</td>\n",
       "      <td>1.58</td>\n",
       "      <td>120.501117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rainity_build_0.2222_1.0_0.5092_24_2.15_0.6156...</td>\n",
       "      <td>model_1_ptm_seed_0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.23</td>\n",
       "      <td>91.73</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.67</td>\n",
       "      <td>5.160515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rainity_build_0.2222_1.0_0.3805_18_2.44_0.0040...</td>\n",
       "      <td>model_1_ptm_seed_0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.65</td>\n",
       "      <td>93.08</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.87</td>\n",
       "      <td>132.663305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rainity_build_0.2285_1.0_0.3425_18_2.47_0.0074...</td>\n",
       "      <td>model_1_ptm_seed_0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.21</td>\n",
       "      <td>81.84</td>\n",
       "      <td>0.57</td>\n",
       "      <td>1.27</td>\n",
       "      <td>5.284478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rainity_build_0.2_1.0_0.2992_20_2.58_0.015600_...</td>\n",
       "      <td>model_1_ptm_seed_0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.77</td>\n",
       "      <td>80.32</td>\n",
       "      <td>0.44</td>\n",
       "      <td>1.67</td>\n",
       "      <td>5.108682</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name               model  \\\n",
       "0  rainity_build_0.2089_0.98_0.4653_19_2.00_0.005...  model_1_ptm_seed_0   \n",
       "1  rainity_build_0.2222_1.0_0.5092_24_2.15_0.6156...  model_1_ptm_seed_0   \n",
       "2  rainity_build_0.2222_1.0_0.3805_18_2.44_0.0040...  model_1_ptm_seed_0   \n",
       "3  rainity_build_0.2285_1.0_0.3425_18_2.47_0.0074...  model_1_ptm_seed_0   \n",
       "4  rainity_build_0.2_1.0_0.2992_20_2.58_0.015600_...  model_1_ptm_seed_0   \n",
       "\n",
       "   recycle   tol  mean_plddt   ptM  rmsd        time  \n",
       "0        3  0.23       86.44  0.66  1.58  120.501117  \n",
       "1        3  0.23       91.73  0.58  0.67    5.160515  \n",
       "2        3  0.65       93.08  0.64  0.87  132.663305  \n",
       "3        3  0.21       81.84  0.57  1.27    5.284478  \n",
       "4        3  2.77       80.32  0.44  1.67    5.108682  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### superfold results to a df\n",
    "superfold_all_df = pd.read_csv(chA_superfold_output_dir + '/reports.txt', skiprows=0, engine='python', sep = \" |:\", names = [\"name\",\"model\",\"xx.1\",\"recycle\",\"xx.2\",\"tol\",\"xx.3\",\"mean_plddt\", \"xx.4\",\"ptM\", \"xx.5\",\"rmsd\",\"xx.6\",\"xx.7\",\"xx.8\",\"xx\",\"time\"])\n",
    "superfold_all_df = superfold_all_df.drop(columns=[\"xx\",\"xx.1\",\"xx.2\",\"xx.3\",\"xx.4\",\"xx.5\",\"xx.6\",\"xx.7\",\"xx.8\"])\n",
    "superfold_all_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "### combine Rosetta metrics with AFfold to a df\n",
    "\n",
    "df = pd.read_csv(f'{sc_dir}/all_scores_test.csv')\n",
    "\n",
    "AF2_lDDT_dict = pd.Series(superfold_all_df['mean_plddt'].values,superfold_all_df['name']).to_dict()\n",
    "AF2_rmsd_dict = pd.Series(superfold_all_df['rmsd'].values,superfold_all_df['name']).to_dict()\n",
    "score_per_res_dict = pd.Series(df['score_per_res'].values,df['description'].str.split(\"/\").str[-1]).to_dict()\n",
    "\n",
    "df['lDDT'] = df['description'].map(AF2_lDDT_dict) \n",
    "df['rmsd'] = df['description'].map(AF2_rmsd_dict)\n",
    "df['score_per_res'] = df['description'].map(score_per_res_dict)\n",
    "df['AF2_path'] = chA_superfold_output_dir + '/' + df['description'] + '_model_1_ptm_seed_0_unrelaxed.pdb'\n",
    "\n",
    "df.to_csv(f'{sc_dir}/all_scores_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### filter monomers\n",
    "dfsub = df[(df['score_per_res']<-2.7) & (df['lDDT']>80)] # safer to filter at plddt>90.\n",
    "dfsub = dfsub.copy()\n",
    "dfsub.loc[:, 'pdb_for_design'] = designed_monomers_all_dir + '/' + dfsub['description'].astype(str)\n",
    "len(dfsub['pdb_for_design'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that some of the AlphaFold predicted backbones would also be good for design, so we also copy them into a staging directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy to filtered designed monomers (AF)+(DM)\n",
    "chA_superfold_output_filtered_dir = run_dir + \"AF/chA_superfold_output_filtered\" ### new_dir\n",
    "os.makedirs(chA_superfold_output_filtered_dir, exist_ok=True)\n",
    "\n",
    "for i,row in dfsub.iterrows():\n",
    "    DM_path = str(row['pdbpath'])\n",
    "    AF2_path = str(row['AF2_path'])\n",
    "    d = str(row['description'])\n",
    "    shutil.copy(AF2_path, f'{chA_superfold_output_filtered_dir}/{d}_AF.pdb')\n",
    "    shutil.copy(DM_path, f'{chA_superfold_output_filtered_dir}/{d}_DM.pdb')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Align monomers for cage design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The designed and alphafold predicted backbones could have drifted from their original position in the capsid from which they were extracted. Below we align them back onto their parent capsids in preperation for designing the full capsid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate cmd to align monomer_AF and monomer_DM back to cage scaffold\n",
    "os.makedirs(aligned_AF_monomers_dir, exist_ok=True)\n",
    "os.makedirs(aligned_DM_monomers_dir, exist_ok=True)\n",
    "\n",
    "cmdsfile = 'cmds/cmds_align-chA_cage'\n",
    "script = f'{current_dir}/scripts/pymol_aln_n_rmsd.py'\n",
    "\n",
    "with open(cmdsfile, 'w') as f_out:\n",
    "    for i,row in dfsub.iterrows():\n",
    "        DM_path = str(row['pdbpath'])\n",
    "        AF2_path = str(row['AF2_path'])\n",
    "        pdb_DM_name = DM_path.split('/')[-1]\n",
    "        pdb_AF2_name = AF2_path.split('/')[-1]\n",
    "        \n",
    "        aligned_AF2_pdb = aligned_AF_monomers_dir + '/' + pdb_DM_name.split('/')[-1].replace('_0001.','_0001_AF2_aligned.')\n",
    "        aligned_DM_pdb = aligned_DM_monomers_dir + '/' + pdb_DM_name.split('/')[-1].replace('_0001.','_0001_DM_aligned.')\n",
    "\n",
    "        \n",
    "        dock =  chA_dir + '/' + str(row['description']).replace('rainity_','').replace('_0001','.pdb')\n",
    "        cmd1 = f\"{pymol} -c {script} --input_pdb1 {AF2_path} --input_pdb2 {dock} --output_pdb {aligned_AF2_pdb}\" ## align AF-models to designs\n",
    "        cmd2 = f\"{pymol} -c {script} --input_pdb1 {DM_path} --input_pdb2 {dock} --output_pdb {aligned_DM_pdb}\" ## align AF-models to designs\n",
    "                \n",
    "        f_out.write(cmd1 + '\\n')\n",
    "        f_out.write(cmd2 + '\\n')\n",
    "    \n",
    "#jupyter_utils.make_submit_file(cmds=cmdsfile, submitfile=submitfile, group_size=1000, mem='2G', queue='short')\n",
    "#p = subprocess.Popen(['sbatch', submitfile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RosettaDesign cages with AF and DM monomers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are finally ready to perform the full design of the capsids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### input chainA\n",
    "Aligned_AF_chA_pdbs = glob.glob(aligned_AF_monomers_dir + '/*.pdb')\n",
    "Aligned_DM_chA_pdbs = glob.glob(aligned_DM_monomers_dir + '/*.pdb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design cage with interface pssm (DM = DesignModel)\n",
    "###############################\n",
    "os.makedirs(predesigned_DM_cages_dir, exist_ok=True)\n",
    "\n",
    "xml = f'{current_dir}/scripts_protocols/design_cage/design_cage.xml'\n",
    "flags = f'{current_dir}/scripts_protocols/design_cage/flags_cage_design2'\n",
    "\n",
    "cmdsfile = 'cmds/cmds_design_cage_DM'\n",
    "with open(cmdsfile, 'w') as f_cmds:\n",
    "    for pdbidx, Aligned_DM_chA in enumerate(Aligned_DM_chA_pdbs):\n",
    "        pdbname = Aligned_DM_chA.split('/')[-1][:-4]\n",
    "        pssm_name = pdbname.replace('_0001_DM_aligned','').replace('rainity_','')\n",
    "        pssm_f = f'{int_pssm_dir}/{pssm_name}.pssm'\n",
    "            \n",
    "        chainA = f'{aligned_DM_monomers_dir}' + '/' + Aligned_DM_chA.split('/')[-1]\n",
    "        design_folder = f'{predesigned_DM_cages_dir}/{pdbname}'\n",
    "        os.makedirs(design_folder, exist_ok=True)\n",
    "            \n",
    "        cmd = f'{rosexe} -s {chainA} -parser:protocol {xml} @{flags} -parser:script_vars pssm_f={pssm_f} -out:prefix {design_folder}/cage_' \n",
    "            \n",
    "        # setup cmdfile\n",
    "        f_cmds.write(f'{cmd}\\n')\n",
    "\n",
    "#submitfile = cmdsfile+'.sh'\n",
    "#jupyter_utils.make_submit_file(cmds=cmdsfile, submitfile=submitfile, group_size=1, mem='8G', queue='cpu')\n",
    "#p = subprocess.Popen(['sbatch', submitfile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design cage with interface pssm (AF2_chA)\n",
    "###############################\n",
    "os.makedirs(predesigned_AF_cages_dir, exist_ok=True)\n",
    "\n",
    "xml = f'{current_dir}/scripts_protocols/design_cage/design_cage.xml'\n",
    "flags = f'{current_dir}/scripts_protocols/design_cage/flags_cage_design2'\n",
    "\n",
    "cmdsfile = 'cmds/cmds_design_cage_AF2'\n",
    "with open(cmdsfile, 'w') as f_cmds:\n",
    "    for pdbidx, Aligned_AF_chA in enumerate(Aligned_AF_chA_pdbs):\n",
    "        pdbname = Aligned_AF_chA.split('/')[-1][:-4]\n",
    "        pssm_name = pdbname.replace('_0001_AF2_aligned','').replace('rainity_','')\n",
    "        pssm_f = f'{int_pssm_dir}/{pssm_name}.pssm'\n",
    "            \n",
    "        chainA = f'{aligned_AF_monomers_dir}' + '/' + Aligned_AF_chA.split('/')[-1]\n",
    "        design_folder = f'{predesigned_AF_cages_dir}/{pdbname}'\n",
    "        os.makedirs(design_folder, exist_ok=True)\n",
    "            \n",
    "        cmd = f'{rosexe} -s {chainA} -parser:protocol {xml} @{flags} -parser:script_vars pssm_f={pssm_f} -out:prefix {design_folder}/cage_' \n",
    "            \n",
    "        # setup cmdfile\n",
    "        f_cmds.write(f'{cmd}\\n')\n",
    "\n",
    "#submitfile = cmdsfile+'.sh'\n",
    "#jupyter_utils.make_submit_file(cmds=cmdsfile, submitfile=submitfile, group_size=1, mem='8G', queue='cpu')\n",
    "#p = subprocess.Popen(['sbatch', submitfile])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## All scores: Alpha Fold, cage interfaces, and individual interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that remains is to gather all the scores for filtering the set of capsids down, to whichever number we want to characterize experimentally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Collect cage scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get DMcage scores\n",
    "###############################\n",
    "DM_score_paths = glob.glob(predesigned_DM_cages_dir + '/*/*.sc')\n",
    "\n",
    "DM_cage_dfs = []\n",
    "for score_pathsidx, score_path in enumerate(DM_score_paths):\n",
    "    df_f = pd.read_csv(f'{score_path}',skiprows=1,delim_whitespace=True).tail(1) # adding tail here\n",
    "                                                                                 # in case use has been\n",
    "                                                                                 # writing to score.sc\n",
    "                                                                                 # multiple times\n",
    "    DM_cage_dfs.append(df_f)\n",
    "\n",
    "DM_cage_dfs=pd.concat(DM_cage_dfs).dropna()\n",
    "DM_cage_dfs.to_csv(f'{sc_dir}/all_cage_scores_test_DM_1st.csv', index=False)\n",
    "len(DM_cage_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get AFcage scores\n",
    "###############################\n",
    "AF_score_paths = glob.glob( predesigned_AF_cages_dir + '/*/*.sc')\n",
    "\n",
    "AF_cage_dfs = []\n",
    "for score_pathsidx, score_path in enumerate(AF_score_paths):\n",
    "    df_f = pd.read_csv(f'{score_path}',skiprows=1,delim_whitespace=True).tail(1) # adding tail here\n",
    "                                                                                 # in case use has been\n",
    "                                                                                 # writing to score.sc\n",
    "                                                                                 # multiple times\n",
    "    AF_cage_dfs.append(df_f)\n",
    "\n",
    "AF_cage_dfs=pd.concat(AF_cage_dfs).dropna()\n",
    "AF_cage_dfs.to_csv(f'{sc_dir}/all_cage_scores_test_AF_1st.csv', index=False)\n",
    "len(AF_cage_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### AlphaFold cage_monomer lDDT and rmsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract chain A for 1st DM and 1st AF\n",
    "################################################\n",
    "\n",
    "capsid_1st_DM = glob.glob(predesigned_DM_cages_dir + '/*/*pdb')\n",
    "# 1st AF designed cages\n",
    "capsid_1st_AF = glob.glob(predesigned_AF_cages_dir + '/*/*pdb')\n",
    "\n",
    "os.makedirs(predesigned_DM_cages_all_chA_dir, exist_ok=True)\n",
    "os.makedirs(predesigned_AF_cages_all_chA_dir, exist_ok=True)\n",
    "\n",
    "###### extract chain A for 1st DM\n",
    "cmdsfile1 = 'cmds/cmds_extract_chA_from_designed_capsids' ###\n",
    "with open(cmdsfile1, 'w') as f_out:\n",
    "    for pdb in capsid_1st_DM:   ### AF\n",
    "        output_dir = predesigned_DM_cages_all_chA_dir   ### AF\n",
    "        cmd1 = f\"{pymol} -c scripts/extract_chains.py --only_chainA 1 --pdb_in {pdb} --pdb_out {output_dir}/{pdb.split('/')[-1]}\"\n",
    "        f_out.write(cmd1+'\\n')\n",
    "\n",
    "###### extract chain A for 1st AF       \n",
    "cmdsfile2 = 'cmds/cmds_extract_chA_from_designed_capsids' \n",
    "with open(cmdsfile2, 'a') as f_out:\n",
    "    for pdb in capsid_1st_AF:\n",
    "        output_dir = predesigned_AF_cages_all_chA_dir\n",
    "        cmd2 = f\"{pymol} -c scripts/extract_chains.py --only_chainA 1 --pdb_in {pdb} --pdb_out {output_dir}/{pdb.split('/')[-1]}\"\n",
    "        f_out.write(cmd2 +'\\n')\n",
    "                \n",
    "#submitfile = cmdsfile+'.sh'\n",
    "#jupyter_utils.make_submit_file(cmds=cmdsfile, submitfile=submitfile, group_size=1000)\n",
    "#p = subprocess.Popen(['sbatch', submitfile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "### superfold filtered designed monomers to get mean pLDDT and rmsd\n",
    "cage_chA_superfold_output_dir = run_dir + \"AF/cage_chA_superfold_output\" ### new_dir\n",
    "os.makedirs(cage_chA_superfold_output_dir, exist_ok=True)\n",
    "\n",
    "cmdsfile = 'cmds/cmds_superfold'\n",
    "\n",
    "with open(cmdsfile, 'w') as f_out:\n",
    "    f_out.write(superfold_exe)\n",
    "    f_out.close()\n",
    "\n",
    "with open(cmdsfile, 'a') as f_out:\n",
    "    for cage_DM_chA in glob.glob(predesigned_DM_cages_all_chA_dir + '/*pdb'):\n",
    "        cmd = f\"{cage_DM_chA}\"\n",
    "        f_out.write(\" \" + cmd)\n",
    "        \n",
    "with open(cmdsfile, 'a') as f_out:\n",
    "    for cage_AF_chA in glob.glob(predesigned_AF_cages_all_chA_dir + '/*pdb'):\n",
    "        cmd = f\"{cage_AF_chA}\"\n",
    "        f_out.write(\" \" + cmd)\n",
    "\n",
    "with open(cmdsfile, 'a') as f_out:\n",
    "    f_out.write(\" --out_dir \" + cage_chA_superfold_output_dir + \" --models 4 --output_summary\")\n",
    "    f_out.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Collect AlphaFold monomer lDDT rmsd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "### combine Rosetta metrics with AFfold to a df\n",
    "superfold_cage_chA_all_df = pd.read_csv(cage_chA_superfold_output_dir + '/reports.txt', skiprows=0,engine='python', sep = \" |:\", names = [\"name\",\"model\",\"xx.1\",\"recycle\",\"xx.2\",\"tol\",\"xx.3\",\"mean_plddt\", \"xx.4\",\"ptM\", \"xx.5\",\"rmsd\",\"xx.6\",\"xx.7\",\"xx.8\",\"xx\",\"time\"])\n",
    "superfold_cage_chA_all_df = superfold_cage_chA_all_df.drop(columns=[\"xx\",\"xx.1\",\"xx.2\",\"xx.3\",\"xx.4\",\"xx.5\",\"xx.6\",\"xx.7\",\"xx.8\"])\n",
    "\n",
    "DM_cage_dfs = pd.read_csv(f'{sc_dir}/all_cage_scores_test_DM_1st.csv')\n",
    "AF_cage_dfs = pd.read_csv(f'{sc_dir}/all_cage_scores_test_AF_1st.csv')\n",
    "\n",
    "frames = [AF_cage_dfs, DM_cage_dfs]\n",
    "all_cage_dfs = pd.concat(frames)\n",
    "\n",
    "AF2_model_dict = pd.Series(superfold_cage_chA_all_df['model'].values,superfold_cage_chA_all_df['name']).to_dict()\n",
    "AF2_lDDT_dict = pd.Series(superfold_cage_chA_all_df['mean_plddt'].values,superfold_cage_chA_all_df['name']).to_dict()\n",
    "AF2_rmsd_dict = pd.Series(superfold_cage_chA_all_df['rmsd'].values,superfold_cage_chA_all_df['name']).to_dict()\n",
    "AF2_ptM_dict = pd.Series(superfold_cage_chA_all_df['ptM'].values,superfold_cage_chA_all_df['name']).to_dict()\n",
    "AF2_tol_dict = pd.Series(superfold_cage_chA_all_df['tol'].values,superfold_cage_chA_all_df['name']).to_dict()\n",
    "\n",
    "all_cage_dfs['model'] = all_cage_dfs['description'].map(AF2_model_dict)\n",
    "all_cage_dfs['lDDT'] = all_cage_dfs['description'].map(AF2_lDDT_dict) \n",
    "all_cage_dfs['rmsd'] = all_cage_dfs['description'].map(AF2_rmsd_dict)\n",
    "all_cage_dfs['ptM'] = all_cage_dfs['description'].map(AF2_ptM_dict)\n",
    "all_cage_dfs['tol'] = all_cage_dfs['description'].map(AF2_tol_dict)\n",
    "all_cage_dfs['AF2_path'] = cage_chA_superfold_output_dir + '/' + all_cage_dfs['description'] + '_'+ all_cage_dfs['model'] + '_unrelaxed.pdb'\n",
    "\n",
    "all_cage_dfs.to_csv(f'{sc_dir}/all_cage_scores_test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Score and filter on individual vs interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the calculated metrics above, it is crucial to ensure that each capsid forms a sufficient number of high-quality interfaces for successful assembly. Each subunit has the potential to form C2, C3, and C5 interfaces. For optimal capsid assembly, a minimum of two good interfaces is required. To achieve this, we must assess and score each interface individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('test/cage_design/1st_AF_chA_all/interfaces/',\n",
       " 'test/cage_design/1st_DM_chA_all/interfaces/')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_DM_interfaces_dir = predesigned_DM_cages_all_chA_dir + '/interfaces/'\n",
    "extracted_AF_interfaces_dir = predesigned_AF_cages_all_chA_dir + '/interfaces/'\n",
    "extracted_AF_interfaces_dir, extracted_DM_interfaces_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below extracts the C2, C3, and C5 interface for each capsid.\n",
    "cmds = 'cmds/cmds_find_interfaces' \n",
    "with open(cmds, 'w') as f_out:\n",
    "    f_out.write('cd scripts_protocols/find_score_interfaces; ')\n",
    "    f_out.write(f'apptainer run {RL_apptainer} find_interface_types.py \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### score DM interfaces with Rosetta (dimer/trimer/pentamers)\n",
    "outdir = extracted_DM_interfaces_dir + '/scored'\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "extracted_DM_interfaces = glob.glob(extracted_DM_interfaces_dir + '/*.pdb')\n",
    "protocol = 'protocols/score_interfaces.xml'\n",
    "flags = 'protocols/flags_score_interface'\n",
    "\n",
    "cmdsfile = 'cmds/cmds_score_interfaces_DM'\n",
    "with open(cmdsfile, 'w') as f_cmds:\n",
    "    for pdbidx, interface in enumerate(extracted_DM_interfaces):\n",
    "        pdbname = interface.split('/')[-1][:-4]\n",
    "        input_pdb = f'{extracted_DM_interfaces_dir}' + '/' + interface.split('/')[-1]\n",
    "        sc_folder = f'{outdir}/{pdbname}/'\n",
    "        os.makedirs(sc_folder, exist_ok=True)\n",
    "        \n",
    "        cmd = f'{rosexe} -s {input_pdb} -parser:protocol {protocol} @{flags} -out:prefix {sc_folder}'             \n",
    "        f_cmds.write(cmd + '\\n')\n",
    "\n",
    "#submitfile = cmdsfile+'.sh'\n",
    "#jupyter_utils.make_submit_file(cmds=cmdsfile, submitfile=submitfile, group_size=1, mem='6G', queue='cpu')\n",
    "#p = subprocess.Popen(['sbatch', submitfile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### score AF interfaces (dimer/trimer/pentamers)\n",
    "outdir = extracted_AF_interfaces_dir\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "extracted_AF_interfaces = glob.glob(extracted_AF_interfaces_dir + '/*.pdb')\n",
    "protocol = 'protocols/score_interfaces.xml'\n",
    "flags = 'protocols/flags_score_interface'\n",
    "\n",
    "cmdsfile = 'cmds/cmds_score_interfaces_AF'\n",
    "with open(cmdsfile, 'w') as f_cmds:\n",
    "    for pdbidx, interface in enumerate(extracted_AF_interfaces):\n",
    "        pdbname = interface.split('/')[-1][:-4]\n",
    "        input_pdb = f'{extracted_AF_interfaces_dir}' + '/' + interface.split('/')[-1]\n",
    "        sc_folder = f'{outdir}/scored/{pdbname}/'\n",
    "        os.makedirs(sc_folder, exist_ok=True)\n",
    "        \n",
    "        cmd = f'{rosexe} -s {input_pdb} -parser:protocol {xml} @{flags} -out:prefix {sc_folder}'             \n",
    "        f_cmds.write(cmd + '\\n')\n",
    "\n",
    "#submitfile = cmdsfile+'.sh'\n",
    "#jupyter_utils.make_submit_file(cmds=cmdsfile, submitfile=submitfile, group_size=5, mem='6G', queue='cpu')\n",
    "#p = subprocess.Popen(['sbatch', submitfile])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 score files in AF_score_paths\n",
      "Found 23 score files in DM_score_paths\n",
      "Got complete C2, C3, and C5 score sets for 46 pdbs \n"
     ]
    }
   ],
   "source": [
    "#### combine dimer/trimer/pentamer in one df\n",
    "\n",
    "AF_score_paths = glob.glob( extracted_AF_interfaces_dir + '/scored/*_trimer/score.sc')\n",
    "DM_score_paths = glob.glob( extracted_DM_interfaces_dir + '/scored/*_trimer/score.sc')\n",
    "\n",
    "print(f'Found {len(AF_score_paths)} score files in AF_score_paths')\n",
    "print(f'Found {len(DM_score_paths)} score files in DM_score_paths')\n",
    "\n",
    "score_paths =  DM_score_paths + AF_score_paths\n",
    "\n",
    "def get_name(s):\n",
    "    return '_'.join(s.split('/')[-1].split('_')[:-2])\n",
    "\n",
    "dfs = []\n",
    "for score_pathsidx, score_path in enumerate(score_paths):   \n",
    "    # Dimers\n",
    "    dimer_f = score_path.replace('trimer','dimer')\n",
    "    df_2 = pd.read_csv(f'{dimer_f}',skiprows=1,delim_whitespace=True).tail(1)\n",
    "    new_name_map = {x:x+'_2' for x in df_2.columns}\n",
    "    df_2 = df_2.rename(columns=new_name_map)\n",
    "    df_2['name'] = df_2.apply(lambda x: get_name(x['description_2']), axis=1)\n",
    "    \n",
    "    # Trimers\n",
    "    trimer_f = score_path\n",
    "    df_3 = pd.read_csv(f'{trimer_f}',skiprows=1,delim_whitespace=True).tail(1)\n",
    "    new_name_map = {x:x+'_3' for x in df_3.columns}\n",
    "    df_3 = df_3.rename(columns=new_name_map)\n",
    "    df_3['name'] = df_3.apply(lambda x: get_name(x['description_3']), axis=1)\n",
    "\n",
    "    # Pentamers\n",
    "    pentamer_f = score_path.replace('trimer','pentamer')\n",
    "    df_5 = pd.read_csv(f'{pentamer_f}',skiprows=1,delim_whitespace=True).tail(1)\n",
    "    new_name_map = {x:x+'_5' for x in df_5.columns}\n",
    "    df_5 = df_5.rename(columns=new_name_map)\n",
    "    df_5['name'] = df_5.apply(lambda x: get_name(x['description_5']), axis=1)\n",
    "    \n",
    "    # Merge all the data   \n",
    "    df_merge = df_2.merge(df_3)    \n",
    "    df_merge = df_merge.merge(df_5)\n",
    "    dfs.append(df_merge)\n",
    "\n",
    "df_combined = pd.concat(dfs)\n",
    "\n",
    "print(f'Got complete C2, C3, and C5 score sets for {len(df_combined)} pdbs ')\n",
    "df_combined.to_csv(f'{sc_dir}/all_interface_scores_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Combine all scores (cage_monomer_interfaces) in 1 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "### combine all scores\n",
    "df = pd.read_csv(f'{sc_dir}/all_interface_scores_test.csv')\n",
    "\n",
    "All_scores_df = pd.read_csv(f'{sc_dir}/all_cage_scores_test.csv')\n",
    "\n",
    "ddG2_dict = pd.Series(df['ddg_2'].values,df['name']).to_dict()\n",
    "ddG3_dict = pd.Series(df['ddg_3'].values,df['name']).to_dict()\n",
    "ddG5_dict = pd.Series(df['ddg_5'].values,df['name']).to_dict()\n",
    "\n",
    "cms2_dict = pd.Series(df['contact_molecular_surface_2'].values,df['name']).to_dict()\n",
    "cms3_dict = pd.Series(df['contact_molecular_surface_3'].values,df['name']).to_dict()\n",
    "cms5_dict = pd.Series(df['contact_molecular_surface_5'].values,df['name']).to_dict()\n",
    "\n",
    "cms_ap2_dict = pd.Series(df['contact_molecular_surface_ap_target_2'].values,df['name']).to_dict()\n",
    "cms_ap3_dict = pd.Series(df['contact_molecular_surface_ap_target_3'].values,df['name']).to_dict()\n",
    "cms_ap5_dict = pd.Series(df['contact_molecular_surface_ap_target_5'].values,df['name']).to_dict()\n",
    "\n",
    "All_scores_df['cms_2'] = All_scores_df['description'].map(cms2_dict)\n",
    "All_scores_df['cms_3'] = All_scores_df['description'].map(cms3_dict)\n",
    "All_scores_df['cms_5'] = All_scores_df['description'].map(cms5_dict)\n",
    "All_scores_df['cms-ap_2'] = All_scores_df['description'].map(cms_ap2_dict)\n",
    "All_scores_df['cms-ap_3'] = All_scores_df['description'].map(cms_ap3_dict)\n",
    "All_scores_df['cms-ap_5'] = All_scores_df['description'].map(cms_ap5_dict)\n",
    "All_scores_df['ddG_2'] = All_scores_df['description'].map(ddG2_dict)\n",
    "All_scores_df['ddG_3'] = All_scores_df['description'].map(ddG3_dict)\n",
    "All_scores_df['ddG_5'] = All_scores_df['description'].map(ddG5_dict) \n",
    "\n",
    "All_scores_df.to_csv(f'{sc_dir}/all_scores_test_1stround.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Filtering all metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_scores_df = pd.read_csv(f'{sc_dir}/all_scores_test_1stround.csv')\n",
    "assert len(All_scores_df) == len(All_scores_df.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Designs passing filters 2\n"
     ]
    }
   ],
   "source": [
    "#relevant_features = ['binder_delta_sap', 'target_delta_sap', 'contact_molecular_surface', 'contact_molecular_surface_ap_target','contact_molec_sq5_ap_target','ddg', 'interface_buried_sasa','interface_sc','ss_sc','score_per_res', 'interface_sc_median_dist']\n",
    "\n",
    "All_scores_df['total_ddg'] = All_scores_df['ddG_2'] + All_scores_df['ddG_3']*2 + All_scores_df['ddG_5']*2\n",
    "\n",
    "# Criteria for ONE good interfaces \"cms = contact_molecular_surface\" \"cms-ap = contact_molecular_surface_ap_target\"\n",
    "feature2cutoff = {'ddG': -20, \\\n",
    "                  'cms': 180, \\\n",
    "                  'cms-ap': 100} \n",
    "\n",
    "feature2direction = {'ddG': '<', \\\n",
    "                     'cms': '>', \\\n",
    "                     'cms-ap': '>'}\n",
    "\n",
    "def calc_n_good_interfaces(row):\n",
    "    n_good = 0\n",
    "    for interface in ['_2', '_3', '_5']:\n",
    "        criteria_passed = 0\n",
    "        for feature_name in feature2cutoff:\n",
    "            greaterThan = (feature2direction[feature_name]=='>')\n",
    "            feature_name_for_interface = feature_name + interface\n",
    "            feature_score = row[feature_name_for_interface]\n",
    "            if greaterThan:\n",
    "                is_feature_passing = (feature_score > feature2cutoff[feature_name])\n",
    "            else:\n",
    "                is_feature_passing = (feature_score < feature2cutoff[feature_name])\n",
    "            criteria_passed += int(is_feature_passing)\n",
    "        is_good = (len(feature2cutoff) == criteria_passed)\n",
    "        n_good += int(is_good)\n",
    "    return n_good\n",
    "\n",
    "All_scores_df['n_good_interfaces'] = All_scores_df.apply(lambda x: calc_n_good_interfaces(x), axis=1)\n",
    "\n",
    "#### cage individual interface metrics\n",
    "All_scores_df_filtered = All_scores_df[  (All_scores_df['n_good_interfaces']>=2)\\\n",
    "                 \n",
    "#### AF metrics        ###90  ###1         \n",
    "& (All_scores_df['lDDT'] >85)\\\n",
    "& (All_scores_df['rmsd']<2)\\\n",
    "\n",
    "                 \n",
    "#### cage interface metrics   ###30   ###4  ###4   ###0.6\n",
    "& (All_scores_df['sap_score']<30)\\\n",
    "& (All_scores_df['sbuns_all_heavy'] <3)\\\n",
    "& (All_scores_df['vbuns_all_heavy'] <3)\\\n",
    "& (All_scores_df['sc'] >0.63)\\\n",
    "                 \n",
    "                ]\n",
    "# If we see that they have a lot of apolar surface area we could filter on solvent_exposed_sap as well.\n",
    "print(f'Designs passing filters {len(All_scores_df_filtered)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The passing pdbs can be found here:\n",
      "\n",
      "test/cage_design/1st_DM/rainity_build_0.2089_0.98_0.4653_19_2.00_0.005745_0001_DM_aligned/cage_rainity_build_0.2089_0.98_0.4653_19_2.00_0.005745_0001_DM_aligned_0001.pdb\n",
      "test/cage_design/1st_DM/rainity_build_0.2222_1.0_0.5092_24_2.15_0.615602_0001_DM_aligned/cage_rainity_build_0.2222_1.0_0.5092_24_2.15_0.615602_0001_DM_aligned_0001.pdb\n"
     ]
    }
   ],
   "source": [
    "all_designs = glob.glob('test/cage_design/1st_??/*/*pdb')\n",
    "\n",
    "print('The passing pdbs can be found here:\\n')\n",
    "description2designPath = {f.split('/')[-1][:-4]:f for f in all_designs}\n",
    "for description in All_scores_df_filtered['description']:\n",
    "    print(description2designPath[description])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: Order and see if your designs work. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem_obs",
   "language": "python",
   "name": "deepchem_obs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
